{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ffeab86",
   "metadata": {},
   "source": [
    "# Run all cells once, then run `Run Code` section for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63493b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import itertools\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c160a4",
   "metadata": {},
   "source": [
    "## Prepare data files before training Transformer \n",
    "\n",
    "Files will be written in the needed format and saved in the `data/fortransfer/` folder. \n",
    "\n",
    "Also, the `_log.txt` file with statistics about the training data will be written to the `reports/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d47a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitclitics(line):\n",
    "    brokenline = line.split('-')\n",
    "    #proclitic\n",
    "    if '=' in brokenline[0]:\n",
    "        cliticsplit = brokenline[0].split('=')\n",
    "        first = cliticsplit[0]\n",
    "        second = cliticsplit[1]\n",
    "        brokenline[0] = second\n",
    "        brokenline.insert(0, first+'=')\n",
    "    if '=' in brokenline[-1]: # enclitic\n",
    "        cliticsplit = brokenline[-1].split('=')\n",
    "        first = cliticsplit[-2]\n",
    "        second = cliticsplit[-1]\n",
    "        brokenline.append('='+second)\n",
    "    return brokenline\n",
    "\n",
    "def formatting(line):\n",
    "    '''Put language specific processing of text as needed\n",
    "    This only for preprocessing that needs to happens\n",
    "     after 1st training iteration'''\n",
    "    if TEAMCODE == 'bkft':\n",
    "        line = line.lower()\n",
    "        line = line.replace('ó', 'o')\n",
    "        line = line.replace('á', 'a')\n",
    "        line = line.replace('í', 'i')\n",
    "    return line\n",
    "\n",
    "\n",
    "def grouplines(file):\n",
    "    lines = [formatting(line.strip()) for line in open(file, encoding='utf8')] \n",
    "    # get interlinear lines for each sentence\n",
    "    igtgroups = []\n",
    "    curr_igt = []\n",
    "    for line in lines:\n",
    "        if line == '':\n",
    "            igtgroups.append(curr_igt)\n",
    "            curr_igt = []\n",
    "        else:\n",
    "            curr_igt.append(line+EOS)    \n",
    "    return igtgroups\n",
    "\n",
    "\n",
    "def retrieve(igtdata, devsize=0):\n",
    "    devigt = []\n",
    "    if type(igtdata) == list: # dev igt\n",
    "        igt = igtdata\n",
    "    else: \n",
    "        igt = grouplines(igtdata)\n",
    "        # create dev split by sentence\n",
    "        if igtdata.endswith('train'):\n",
    "            devigt = igt[:devsize]\n",
    "            igt = igt[devsize:]\n",
    "    # break lines into words \n",
    "    orig_words = []\n",
    "    morphseg = []\n",
    "    gloss = []\n",
    "    for group in igt:\n",
    "        orig_words.extend(group[0].split())\n",
    "        morphseg.extend(group[1].split())\n",
    "        gloss.extend(group[2].split())\n",
    "    # break into morphemes\n",
    "    seg_words = []\n",
    "    gloss_words = []\n",
    "    for i,word in enumerate(morphseg):\n",
    "        if '=' in word:\n",
    "            seg_words.append(splitclitics(word))\n",
    "            gloss_words.append(splitclitics(gloss[i]))\n",
    "        else:\n",
    "            seg_words.append(word.split('-'))\n",
    "            gloss_words.append(gloss[i].split('-'))\n",
    "           \n",
    "    return orig_words, seg_words, gloss_words, devigt\n",
    "\n",
    "\n",
    "def join_igt(igttuples):\n",
    "    joined = []\n",
    "    for word in igttuples:\n",
    "        joined.append(' '.join((morph+'%%'+gloss for morph,gloss in word)))\n",
    "    return joined\n",
    "\n",
    "\n",
    "def prepoutput(segs, glosses):\n",
    "    if TASK == SEG_GLS:\n",
    "        seggls = [list(zip(seg, glosses[i])) for i,seg in enumerate(segs)]\n",
    "        return join_igt(seggls)\n",
    "    else:\n",
    "        return [' '.join(seg) for seg in segs]\n",
    "\n",
    "\n",
    "def toupload(datasplit, indata, outdata):\n",
    "    filename = TRANSFERDIR+datasplit+KEY+ITERATION\n",
    "    if outdata: \n",
    "        with open(filename+'.output', 'w', encoding='utf8') as e:\n",
    "            e.write('\\n'.join(outdata))\n",
    "    indata = [' '.join(word) for word in indata]\n",
    "    with open(filename+'.input', 'w', encoding='utf8') as e:\n",
    "        e.write('\\n'.join(indata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f50bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logstats(trainout):\n",
    "    # List all \"tags\" in training data\n",
    "    tokens = len(trainout)\n",
    "    \n",
    "    taglist = []\n",
    "    for word in trainout:\n",
    "        taglist.extend(word.split())\n",
    "    tagset = list(set(taglist))\n",
    "    sorted_tags = sorted(Counter(taglist).items(), key=lambda x:x[1], reverse=True)\n",
    "    sort_string = ''\n",
    "    for tag,val in sorted_tags:\n",
    "        sort_string += \"{: <10}\\t{}\\n\".format(tag, val)\n",
    "    \n",
    "    logstring = str(datetime.datetime.now()) + '\\n\\n'    \n",
    "    tokenreport = str(tokens) + \" training tokens\"  + '\\n\\n'\n",
    "    tagreport = \"SEG_GLS in training data:\\n\" + sort_string  + '\\n'\n",
    "    #print(tagreport) \n",
    "    #print(tokenreport)\n",
    "    \n",
    "    msg = logstring+NOTES+tokenreport+tagreport\n",
    "    filepath = REPORTDIR+KEY+ITERATION\n",
    "    with open(filepath+'_log.txt', 'w', encoding='utf8') as l:\n",
    "        l.write(filepath+'\\n\\n'+ msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1fa07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare():\n",
    "    '''get data from files for printing and sharing'''\n",
    "    X_test, segs_test, gloss_test, __ = retrieve(DATADIR+KEY+ITERATION+'.test')\n",
    "    X_train, segs_train, gloss_train, dev = retrieve(DATADIR+KEY+ITERATION+'.train', devsize=len(X_test))\n",
    "    X_dev, segs_dev, gloss_dev, __ = retrieve(dev)\n",
    "    to_predict, __, __, __ = retrieve(DATADIR+KEY+PREVITERATION+'.predict')\n",
    "    \n",
    "    print(len(X_train), len(segs_train), len(gloss_train))\n",
    "    print(len(X_dev), len(segs_dev), len(gloss_dev))\n",
    "    print()\n",
    "    \n",
    "    y_train = prepoutput(segs_train, gloss_train)\n",
    "    y_dev = prepoutput(segs_dev, gloss_dev)\n",
    "    y_test = prepoutput(segs_test, gloss_test)\n",
    "    \n",
    "    # write files for transformer\n",
    "    toupload('predict.', to_predict, None)\n",
    "    toupload('train.', X_train, y_train)\n",
    "    toupload('dev.', X_dev, y_dev)\n",
    "    toupload('test.', X_test, y_test)\n",
    "    \n",
    "    # Log training data statistics, get list of classes\n",
    "    logstats(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e38db",
   "metadata": {},
   "source": [
    "# Post process after training Transformer\n",
    "\n",
    "#### reformat data for editing in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef86143a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def confusionMatrix(gold_tags, predicted_tags):\n",
    "    '''builds and display a confusion matrix so we \n",
    "    can evaluate where our tagger is making wrong \n",
    "    predictions, after we test a POS tagger'''\n",
    "    \n",
    "    alpha_taglist = sorted(set(gold_tags))\n",
    "    confusion_matrix = metrics.confusion_matrix(gold_tags,predicted_tags,labels=alpha_taglist,normalize=\"true\")\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix,display_labels=alpha_taglist)\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (17,17)\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    disp.plot(colorbar=False)\n",
    "    plt.show() # display below\n",
    "    #save as file\n",
    "    plt.savefig(REPORTDIR+KEY+ITERATION+'_matrix.png')    \n",
    "    \n",
    "    matrixstring = '{0:5s}'.format(' ') + '\\t'.join(['{0:^4s}'.format(tag) for tag in alpha_taglist]) + '\\n'\n",
    "    for i,row in enumerate(confusion_matrix):\n",
    "        cols = '\\t'.join(['{:.2f}'.format(round(col,2)) for col in row])\n",
    "        matrixstring+='{0:6s}'.format(alpha_taglist[i]) + cols + '\\n'\n",
    "    \n",
    "    return matrixstring\n",
    "\n",
    "def pad(test_gold, test_guess):\n",
    "    '''Need same number of annotations on each line for results'''\n",
    "    for i,line in enumerate(test_gold):\n",
    "        goldlen = len(line)\n",
    "        guesslen = len(test_guess[i])\n",
    "        # underguessed number of segments\n",
    "        if goldlen > guesslen:\n",
    "            diff = goldlen - guesslen\n",
    "            padding = ['@@@']*diff\n",
    "            test_guess[i] = test_guess[i] + padding\n",
    "        else: # guess too many segments\n",
    "            diff = guesslen - goldlen\n",
    "            padding = ['NOMORPHEME']*diff\n",
    "            test_gold[i] = line + padding\n",
    "    return test_gold, test_guess\n",
    "    \n",
    "def logResults(test_gold, test_guess):\n",
    "    test_gold, test_guess = pad(test_gold, test_guess)\n",
    "    flat_testgold = list(itertools.chain.from_iterable(test_gold))\n",
    "    flat_testpredict = list(itertools.chain.from_iterable(test_guess))\n",
    "    classreport = metrics.classification_report(flat_testgold, flat_testpredict, zero_division=0.0)\n",
    "    #confusionreport = confusionMatrix(flat_testgold, flat_testpredict)\n",
    "    time = str(datetime.datetime.now())\n",
    "    #report = '\\n\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n'.format(classreport, confusionreport)\n",
    "    report = '\\n\\nClassification Report\\n\\n{}'.format(classreport)\n",
    "    with open(REPORTDIR+KEY+ITERATION+'_results.txt', 'w', encoding='utf8') as R:\n",
    "        R.write(time + report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caad110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resegment(wordannotation):\n",
    "    # join segs and glosses into words with hyphens\n",
    "    # remove hyphens from clitics\n",
    "    return '-'.join(wordannotation).replace('-=', '=').replace('=-', '=')\n",
    "\n",
    "\n",
    "def regroup(prediction_output):\n",
    "    '''reformat words, split glosses from segments if needed\n",
    "    recreate sentence lines with scores and annotations'''\n",
    "    \n",
    "    predictwords = [word.strip() for word in open(TRANSFERDIR+'predict.'+KEY+ITERATION+'.input', encoding='utf8')]\n",
    "    \n",
    "    scores = []\n",
    "    sentences = []\n",
    "    annotations = []\n",
    "    sent_score = 0.0\n",
    "    sent_words = []\n",
    "    sent_annotats = []\n",
    "    \n",
    "    for i,line in enumerate(prediction_output):\n",
    "        h, score, prediction = line.split('\\t')\n",
    "        word = ''.join(predictwords[i].split()) #remove spaces\n",
    "        if TASK == SEG_GLS:\n",
    "            annotation = list(zip(*[mg.split('%%') for mg in prediction.split()]))\n",
    "            annotation = (resegment(annotation[0]), resegment(annotation[1]))\n",
    "        else:\n",
    "            annotation = resegment(prediction.split())\n",
    "        #recreate lines\n",
    "        if word == '@EOS@':\n",
    "            scores.append(sent_score/len(sent_words))\n",
    "            sentences.append(sent_words)\n",
    "            annotations.append(sent_annotats)\n",
    "            sent_score = 0.0\n",
    "            sent_words = []\n",
    "            sent_annotats = []\n",
    "        else:\n",
    "            sent_score+=float(score)\n",
    "            sent_words.append(word)\n",
    "            sent_annotats.append(annotation)\n",
    "            \n",
    "    return list(zip(scores, sentences, annotations))\n",
    "\n",
    "def predictFile(sorted_scored_lines):\n",
    "    '''remove scores; \n",
    "    write new predict file for editing'''\n",
    "    \n",
    "    newpredict = []\n",
    "    for line in sorted_scored_lines:\n",
    "        s, text, annotations = line\n",
    "        newline = ' '.join(text)+'\\n'\n",
    "        if TASK == SEG_GLS:\n",
    "            annotations = list(zip(*annotations))\n",
    "            newline+= ' '.join(annotations[0]) + '\\n'\n",
    "            newline+= ' '.join(annotations[1]) + '\\n'\n",
    "        else:\n",
    "            newline+= ' '.join(annotations) + '\\n'\n",
    "        newpredict.append(newline)\n",
    "        \n",
    "    with open(DATADIR+KEY+ITERATION+'.predict', 'w', encoding='utf8') as new:\n",
    "        new.write('\\n'.join(newpredict))\n",
    "        \n",
    "def reformat(prediction_output):\n",
    "    # predictions file of interlinear annotations\n",
    "    scored_lines = regroup(prediction_output)\n",
    "    scored_lines.sort()\n",
    "    predictFile(scored_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd6763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess():\n",
    "    test_guess = [line.strip().split() for line in open(TRANSFERDIR+KEY+ITERATION+'.testpredict', encoding='utf8')]\n",
    "    test_gold = [line.strip().split() for line in open(TRANSFERDIR+'test.'+KEY+ITERATION+'.output', encoding='utf8')]\n",
    "    prediction_output = [line.strip() for line in open(TRANSFERDIR+KEY+ITERATION+'.confidence', encoding='utf8')]\n",
    "\n",
    "    logResults(test_gold, test_guess)\n",
    "    reformat(prediction_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3522cb2",
   "metadata": {},
   "source": [
    "# Run Code (all cells below)\n",
    "\n",
    "During the Machine-in-the-Loop activity help team members through these steps.\n",
    "\n",
    "    - Preprocess data:\n",
    "        -- Check data file names are updated to new iteration number\n",
    "        -- Keep notes of changes to data.\n",
    "    - Adjust and prepare to run code that trains new model:\n",
    "        -- Update variables.\n",
    "        -- Add notes or comments in `NOTES` variable, if desired\n",
    "        -- Run the `prepare()` function\n",
    "        -- Eyeball log file\n",
    "    - Train, test, predict:\n",
    "        -- Upload files to server\n",
    "        -- Run transformer script to train, test, and predict\n",
    "        -- Download predictions and test predictions files\n",
    "    - Evaluate results by running `postprocess()` function\n",
    "    \n",
    "### Update variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "483d4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS = ' @EOS@'\n",
    "\n",
    "# Update first time for each task\n",
    "TEAMCODE = 'bkft'\n",
    "MODEL = 'Trans'\n",
    "SEG_GLS = '_igt'\n",
    "TASK = SEG_GLS\n",
    "\n",
    "# Update every iteration\n",
    "PREVITERATION = '2'\n",
    "ITERATION = '3'\n",
    "NOTES = 'no changes to test data'\n",
    "\n",
    "# File and folder names\n",
    "DATADIR = r'./'+TEAMCODE+'/data/'\n",
    "TRANSFERDIR = r'./'+TEAMCODE+'/fortransfer/'\n",
    "REPORTDIR = r'./'+TEAMCODE+'/reports/'\n",
    "\n",
    "KEY = TEAMCODE+TASK+MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ca2123",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "777765e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546 546 546\n",
      "297 297 297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc989a3",
   "metadata": {},
   "source": [
    "### Uploade, run Transformer, download\n",
    "\n",
    "1. Upload files from `fortransfer/` folder to server with \n",
    "    \n",
    "    Note: You can delete these file from the `fortransfer/` folder once they are safely uploaded..\n",
    "    \n",
    "2. Train, test, and predict with Transformer\n",
    "3. Download the `.confidence` and the `.testpredict` files to `fortransfer/` directory.\n",
    "\n",
    "### Evalute and postprocess results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8333fb65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "postprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dab0d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
